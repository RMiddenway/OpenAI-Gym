{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIT215 Artificial and Computational Intelligence Final Project\n",
    "## October 2020\n",
    "## Roger Middenway\n",
    "\n",
    "This project uses the Frozen Lake 8x8 environment from OpenAI Gym. The Frozen Lake environment involves an 8x8 grid which consists of ice, holes, and a goal. The agent must traverse the map from the upper left corner, to the goal in the lower right, without falling into a hole. The twist is that the ice is slippery, causing the agent to move in a random, but non-inverse direction to the one chosen. \n",
    "\n",
    "The stochastic nature of this environment proves a challenge for reinforcement learning. We'll attempt to build a model using the Q Learning time difference model, chosen because of the discrete nature of the environment. We'll also compare the performance of a SARSA model, a time difference model very similar to Q Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n"
     ]
    }
   ],
   "source": [
    "# Create the frozen lake environment, check its visualisation\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the render above, the map consists of the following:\n",
    "red square - agent\n",
    "F - ice\n",
    "H - hole\n",
    "G - goal\n",
    "\n",
    "Now we create a function which prints each step of a stored set of frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play back resulting path of previous cell\n",
    "def print_frames(frames, limit=np.inf):\n",
    "    \n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.2)\n",
    "        if i > limit:\n",
    "              break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the behaviour of an agent making random decisions, we use the action_space.sample() function. Note that the agent's actual movement doesn't always correspond with the intended movement printed above the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFF\u001b[41mH\u001b[0mFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "Timestep: 60\n",
      "State: 19\n",
      "Action: 3\n",
      "Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Run an episode, applying a random action at each step\n",
    "frames = []\n",
    "\n",
    "env.reset()\n",
    "\n",
    "while True:\n",
    "    count += 1\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "    })\n",
    "    if done:\n",
    "        print_frames(frames)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning\n",
    "\n",
    "In order to implement the Q Learning algorithm, a Q table must first be created, representing the value of each action at each possible state. This will be initialised as a 2D array of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise a q table, in a separate cell so it can be built upon later\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q Learning model takes 3 hyperparameters:\n",
    "alpha controls the rate at which new information is integrated into the model\n",
    "gamma controls the weighting placed on future rewards as opposed to instant rewards\n",
    "epsilon controls the tendency of the model to explore a random step rather than the optimal one\n",
    "\n",
    "The values used here are based on tuning the model over several iterations.\n",
    "\n",
    "The Q Learning model will now be trained over 100000 episodes At each step in each episode, the Q Learning algorithm will: \n",
    "1. Choose and perform an action based on the Q table values (or at random if the epsilon check triggers).\n",
    "2. Apply the reward function\n",
    "3. Calculate a new reward value for the current action and current state, and update the Q Table. The formula used is: new value = old value + alpha * (reward + gamme * max value of next state - old value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "CPU times: user 2min 18s, sys: 18.8 s, total: 2min 36s\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Set hyperparameters\n",
    "alpha = 0.2 # learning speed, size of effect new reward has on weight\n",
    "gamma = 0.9 # importance given to future rewards\n",
    "epsilon = 0.1 # exploration tendency\n",
    "\n",
    "# Initialise analysis variables\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "reward_count = 0\n",
    "\n",
    "# Run 100000 episodes for training\n",
    "for i in range(1, 100001):\n",
    "    # Initialise environment, epochs, penalties and reward\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    done = False\n",
    "\n",
    "    \n",
    "    # Until done returns positive, make a random move based on epsilon, or find the best move based\n",
    "    # on the q table.\n",
    "    while not done:\n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Apply reward function\n",
    "        if reward:\n",
    "            reward_count += 1\n",
    "        reward *= 10\n",
    "        if done and not reward:\n",
    "            reward -= 1\n",
    "        reward -= 0.1\n",
    "        \n",
    "        # Update Q table based on alpha and gamma\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = old_value + alpha * (reward + gamma * next_max - old_value)\n",
    "        q_table[state, action] = new_value\n",
    "        \n",
    "        # Note a penalty if the penalty is below a certain threshold (NB: Not used in this version)\n",
    "        if reward <= -0.5:\n",
    "            penalties += 1\n",
    "        \n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for comparison, the same thing is done for the SARSA algorithm. The core difference between SARSA and Q Learning is that, where Q Learning takes the maximum value from the next state's Q table, SARSA actually makes the next move and uses the value received. This means that the value SARSA uses for the next action is subject to epsilon triggering a random move, so it may not receive the highest possible value. This results in a more conservative convergence of the Q table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "CPU times: user 1min 41s, sys: 15.8 s, total: 1min 57s\n",
      "Wall time: 1min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create q table for SARSA, epochs and penalties\n",
    "q_table_sarsa = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    # Initialise environment, epochs, penalties and reward\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    # Choose initial action\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(q_table_sarsa[state])\n",
    "    \n",
    "    # Until done returns positive, make a random move based on epsilon, or find the best move based\n",
    "    # on the q table.\n",
    "    while not done:\n",
    "            \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            next_action = env.action_space.sample()\n",
    "        else:\n",
    "            next_action = np.argmax(q_table_sarsa[next_state])\n",
    "            \n",
    "         # Apply reward adjustments\n",
    "        if reward:\n",
    "            reward_count += 1\n",
    "        reward *= 10\n",
    "        if done and not reward:\n",
    "            reward -= 1\n",
    "        reward -= 0.1\n",
    "        \n",
    "        # Update Q table based on alpha and gamma\n",
    "        old_value = q_table_sarsa[state, action]\n",
    "        next_value = q_table_sarsa[next_state, next_action]\n",
    "        \n",
    "        new_value = old_value + alpha * (reward + gamma * next_value - old_value)\n",
    "        q_table_sarsa[state, action] = new_value\n",
    "        \n",
    "        # Note a penalty if the penalty is below a certain threshold (NB: Not used in this version)\n",
    "        if reward <= -0.5:\n",
    "            penalties += 1\n",
    "        \n",
    "        state = next_state\n",
    "        action = next_action\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Learning model performance\n",
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 40.96\n",
      "Average penalties per episode: 0.46\n",
      "Success count: 12.0\n",
      "SARSA model performance\n",
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 45.5\n",
      "Average penalties per episode: 0.29\n",
      "Success count: 6.0\n"
     ]
    }
   ],
   "source": [
    "# Run 100 episodes based on q table generated in previous cell\n",
    "\n",
    "\n",
    "\n",
    "# Set which algorithm's results to use\n",
    "#table = q_table_sarsa\n",
    "#table = q_table\n",
    "\n",
    "def test_model(episodes, table):\n",
    "    # Run through set number of episodes, referring to the q table for the correct move\n",
    "    # NB: values are no longer being updated, and no epsilon is used\n",
    "    frames = []\n",
    "    total_epochs, total_penalties = 0, 0\n",
    "    success_count = 0\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        epochs, penalties, reward = 0, 0, 0\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if epochs > 50:\n",
    "                break\n",
    "            action = np.argmax(table[state])\n",
    "            state, reward, done, info = env.step(action)\n",
    "            success_count += reward\n",
    "            if done and reward == 0:\n",
    "                reward -= 0.5\n",
    "            frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "        })\n",
    "            if reward <= -0.5:\n",
    "                penalties += 1\n",
    "\n",
    "            epochs += 1\n",
    "\n",
    "        total_penalties += penalties\n",
    "        total_epochs += epochs\n",
    "\n",
    "    print(f\"Results after {episodes} episodes:\")\n",
    "    print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "    print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
    "    print('Success count:', success_count)\n",
    "    return frames\n",
    "print('Q Learning model performance')\n",
    "ql_frames = test_model(100, q_table)\n",
    "print('SARSA model performance')\n",
    "sars_frames = test_model(100, q_table_sarsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this data we see that both models perform roughly the same although Q Learning has a slight edge, on average using fewer time steps per episode and achieving more successful traversals, however, it also incurs more penalties on average.\n",
    "\n",
    "It should also be noted that the success rate of these models is around 10%. This alarming out come can be attributed to the random nature of the environment and the layout of the map. The best path to the goal is a narrow corridor on the right hand side, which goes past two holes. If we play back the episodes of the trained models' traversals, its behaviour will show us how it still has trouble finding the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFFFFFF\n",
      "FFFFFFF\u001b[41mF\u001b[0m\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "\n",
      "Timestep: 102\n",
      "State: 15\n",
      "Action: 2\n",
      "Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Play back episodes\n",
    "\n",
    "print_frames(ql_frames, 100)\n",
    "print_frames(sars_frames, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple visualisation shows us the dominant action (that with the highest value) for each state in the Q table. Both models show a tendency to avoid the lower right hand side of the board, which is to be expected given that the optimal route is via the left. However, notice that the optimal actions for the lower right hand squares, just above the goal, involve walking off the map. Why do they not try to move towards the goal? \n",
    "\n",
    "This is because the random nature of the moves, which can adjust the direction by plus or minus 90 degrees, means that the safest method to get past these holes and to the reach the goal, is to move in the only direction that won't result in falling into the hole. Whether the agent reaches the goal or not, then becomes a matter of chance. This explains the low success rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q LEARN\n",
      ">>A>>>><\n",
      "AAAAA>V>\n",
      "AA<O>V>V\n",
      "AAAV<O>V\n",
      "AAAO>V>V\n",
      ">OOV><O>\n",
      "<OV<O>O>\n",
      ">V<OVVVO\n",
      "--------\n",
      "SARSA\n",
      ">>>>>AV>\n",
      "AAAA>V>>\n",
      "AA<O>A>>\n",
      "A<<V<OA>\n",
      "AA<O>V>>\n",
      ">OO>A<O>\n",
      "<OV<O>OV\n",
      "<V<O>VVO\n"
     ]
    }
   ],
   "source": [
    "# Print visualisation of dominant actions for each state in q table\n",
    "\n",
    "def print_dir_map(table):\n",
    "    directions = ['<','V','>','A']\n",
    "    direction_matrix = [directions[np.argmax(state)] for state in table]\n",
    "    for i in range(len(table)):\n",
    "        if q_table[i][0] == 0:\n",
    "            direction_matrix[i] = 'O'\n",
    "    for i in range (8):\n",
    "        line = ''\n",
    "        for j in range(8):\n",
    "            line += direction_matrix[i * 8 + j]\n",
    "        print(line)\n",
    "\n",
    "print('Q LEARN')\n",
    "print_dir_map(q_table)\n",
    "print('-'*8)\n",
    "print('SARSA')\n",
    "print_dir_map(q_table_sarsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "This exploration shows the slight differences between the Q Learning and SARSA reinforcement learning algorithms. It also shows the ways in which a RL model adapts to a stochastic environment, opting for safer choices rather than direct paths. This could perhaps be a result of the reward function, however, as to whether adjusting the reward function to create a less risk averse model would lead to better results, would require further testing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
